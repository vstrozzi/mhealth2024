{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template for the submission. You can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# TEAM NAME ON KAGGLE\n",
    "# Group33\n",
    "\n",
    "# GROUP NUMBER\n",
    "# \"group_33\"\n",
    "\n",
    "# TEAM MEMBERS (E-MAIL, LEGI, KAGGLE USERNAME):\n",
    "# \"vstrozzi@ethz.ch\", \"19-924-596\", \"vstrozzi\" \n",
    "# \"hdurand@ethz.ch\", \"18-815-761\", \"hlneds\"\n",
    "# \"nimholz@ethz.ch\", \"19-929-637\", \"nadineimholz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.signal import stft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, multilabel_confusion_matrix\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "import xgboost as xgb \n",
    "from scipy.signal import find_peaks, stft, istft\n",
    "import xgboost as xgb \n",
    "from scipy.signal import butter, lfilter\n",
    "# You may change the mhealth_activity module but your algorithm must support the original version\n",
    "from mhealth_activity import Recording, Trace, Activity, WatchLocation, Path\n",
    "\n",
    "pd.options.display.max_seq_items = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walking Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 200\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def plot_signal(signal, title, ylabel, sampling_rate=SAMPLING_RATE, peaks=[]):\n",
    "    x = np.linspace(0, len(signal) / sampling_rate, len(signal))\n",
    "    t = pd.to_datetime(x, unit='s')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(t if len(peaks) == 0 else np.linspace(0, len(signal), len(signal)) , signal)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Time [min:sec]' if len(peaks) == 0 else \"Indexes\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%M:%S'))\n",
    "    \n",
    "    # Plot peaks if given\n",
    "    if len(peaks) != 0:\n",
    "        plt.plot(peaks, signal[peaks], \"x\", color=\"green\")\n",
    "    plt.show()\n",
    "\n",
    "def get_signal_mag(ax, ay, az):\n",
    "\n",
    "    return (ax**2 + ay**2 + az**2)**0.5\n",
    "\n",
    "# Get magnitude walk\n",
    "def get_signal_walk(path):\n",
    "    recording = Recording(path)\n",
    "\n",
    "    # Magnitude\n",
    "    magn = get_signal_mag(recording.data['ax'].values, recording.data['ay'].values,  recording.data['az'].values)\n",
    "    \n",
    "    \n",
    "    return magn, 0\n",
    "\n",
    "# Clean low frequencies, RFFT since signals contain only real values (+faster)\n",
    "def algo_walk(acc_phase, label = 0, wind_length= 7.6, mov_avg_wind_s= 0.85, std_thr= 0.15000000000000002, std_wind= 0.65, freq_max_hz= 3.0, prom = 1.65, sampling_rate = SAMPLING_RATE, show=False):\n",
    "    # Number samples\n",
    "    N = len(acc_phase)\n",
    "    \n",
    "    # Filter too high frequency out\n",
    "    acc_phase = butter_bandpass_filter(acc_phase, 1, 3, sampling_rate)\n",
    "    # Eval moving average over window\n",
    "    mov_avg_wind = int(mov_avg_wind_s*sampling_rate)\n",
    "    numbers_series = pd.Series(acc_phase)\n",
    "\n",
    "    windows = numbers_series.rolling(mov_avg_wind, center=True)\n",
    "    moving_averages = windows.mean()\n",
    "\n",
    "    # Set first null entries from the list to zero\n",
    "    moving_averages = moving_averages.fillna(0)\n",
    "    acc_filt = moving_averages.to_numpy()\n",
    "\n",
    "    # Plot filered signal\n",
    "    if show:\n",
    "        plot_signal(acc_phase[40*sampling_rate:45*sampling_rate], 'Magnitude Acc Original', 'Amplitude')\n",
    "        plot_signal(acc_filt[40*sampling_rate:45*sampling_rate], 'Magnitude Acc Cleaned', 'Amplitude')\n",
    "\n",
    "    # Get peaks every windows sec\n",
    "    pred = 0\n",
    "\n",
    "    # Walking or movement detection first\n",
    "    step = int(std_wind*sampling_rate)\n",
    "    moving = np.empty(N)\n",
    "    for i in range(step, N - step):\n",
    "        std_l = lambda val: np.std(val[int(i - step):int(i + step)]) > std_thr\n",
    "        moving[i + step]  = std_l(acc_phase)\n",
    "            \n",
    "    # Copy for first and last\n",
    "    moving[0:step] = moving[step:2*step]\n",
    "    moving[-step:]= moving[-2*step:-step]\n",
    "    if show:\n",
    "        plot_signal(moving, \"Accel Signal\", \"Moving\")\n",
    "        plot_signal(acc_phase, \"Accel Signal original\", \"Moving\")\n",
    "    \n",
    "    \n",
    "    wind_step = int(wind_length*sampling_rate)\n",
    "    for i in range(0, N, wind_step):\n",
    "        acc_wind_filt = acc_filt[i:(i+ wind_step)]\n",
    "        acc_wind_orig = acc_phase[i:(i+ wind_step)]\n",
    "        moving_wind = moving[i:(i+ wind_step)]\n",
    "\n",
    "        # Normalize\n",
    "        # Here you can modify freely the parameters of find peak\n",
    "        # Minimum distance for our peaks is len_signal_per_minute/max_acc_beat (30 seconds)\n",
    "        distance = 1/freq_max_hz*sampling_rate\n",
    "        prominence = (np.max(acc_wind_filt) - np.min(acc_wind_filt))/prom\n",
    "        peaks, _ = find_peaks(acc_wind_filt, height = np.mean(acc_wind_filt), distance=distance, prominence=prominence)\n",
    "        \n",
    "        # Keeps peaks only where energy is not too low\n",
    "        peaks = [peak for peak in peaks if moving_wind[peak]]\n",
    "        # Find peaks with double heel, remove half\n",
    "        pred += len(peaks)\n",
    "        \n",
    "\n",
    "        # Plot example in range\n",
    "        if i == 8*wind_step and show:\n",
    "            plot_signal(acc_wind_orig, 'Acc Orig from {} to {} s'.format(i*sampling_rate, (i+ 10)*sampling_rate), 'Amplitude', peaks=peaks)\n",
    "            plot_signal(acc_wind_filt, 'Acc Filt from {} to {} s'.format(i*sampling_rate, (i+ 10)*sampling_rate), 'Amplitude', peaks=peaks)\n",
    "            plot_signal(moving_wind, 'Acc wrist Filt from {} to {} s'.format(i*sampling_rate, (i+ 10)*sampling_rate), 'Amplitude', peaks=peaks)\n",
    "\n",
    "    # Print prediction of score if given heart rate ground truth\n",
    "    if label != 0:   \n",
    "        print(\"We have found {} compared to the gt of {} steps\".format(pred, label))\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_DIFF_SAMPLE_RATE = [\"mx\", \"my\", \"mz\", \"altitude\"]\n",
    "MEASURES = [\"mean\", \"std\", \"energy\", \"max_freq\"]\n",
    "SENSOR_CORR = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\", \"mx\", \"my\", \"mz\", \"altitude\"]\n",
    "METRICS = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\", \"mx\", \"my\", \"mz\", \"altitude\", \"Steps\", \"StepsWindow\"]\n",
    "MAX_LENGTH = 919.935\n",
    "\n",
    "\n",
    "# Get features per sample across windows\n",
    "def features_per_sample(df):\n",
    "    mean = df.groupby('Sample').mean()\n",
    "    mean = mean.add_prefix(\"mean_\")\n",
    "    std = df.groupby('Sample').std()\n",
    "    std = std.add_prefix(\"std_\")\n",
    "\n",
    "    quantile25 = df.groupby('Sample').quantile(q=0.25)\n",
    "    quantile25 = quantile25.add_prefix(\"q25_\")\n",
    "\n",
    "    quantile75 = df.groupby('Sample').quantile(q=0.75)\n",
    "    quantile75 = quantile75.add_prefix(\"q75_\")\n",
    "\n",
    "    df_final =  pd.concat([mean, std, quantile25, quantile75], axis = 1)\n",
    "    return df_final \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only features. Attention, features_corr need to give feats in correct order ex. ax, az, gx, gy, mz\n",
    "def drop_features(df, features, features_corr, measures):\n",
    "    corr_feats = [x for x in df.columns for y in features_corr if y in x]\n",
    "    features_measures = [x for x in df.columns for y in features if y in x]\n",
    "    measures_drop = [x for x in df.columns for y in measures if y in x]\n",
    "    return df.drop(features_measures + corr_feats + measures_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recall corr is standardized\n",
    "# def drop_low_corr(df, corr_mean_treshold  = 0.01, corr_std_treshold = 0.25):\n",
    "#     corr_names = [x + \"_\" + y +\"_\" + \"corr\" for i, x in enumerate(SENSOR_CORR) for y in SENSOR_CORR[(i+1):]]\n",
    "#     low_corr_feats = []\n",
    "#     for corr_col in corr_names:\n",
    "#         if df[corr_col].abs().mean() < corr_mean_treshold or df[corr_col].std() < corr_std_treshold :\n",
    "#             df = df.drop([corr_col], axis=1)\n",
    "#             low_corr_feats.append(corr_col)\n",
    "\n",
    "#     return df, low_corr_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the df to have at least max_window elements per sample, where the padding is with 0\n",
    "def extend_df_windows(df, data_every = 30, wind_length_s = 60, wind_overlap_per = 0.5):\n",
    "    wind_step = wind_length_s*wind_overlap_per\n",
    "    max_window = int((MAX_LENGTH)//wind_step)\n",
    "\n",
    "    # Create max_window consecutive rows\n",
    "    df['g'] = df.groupby('Sample').cumcount()\n",
    "    mux = pd.MultiIndex.from_product([df['Sample'].unique(), range(max_window)], names=('Sample','g'))\n",
    "\n",
    "    df = (df.set_index(['Sample','g'])\n",
    "        .reindex(mux, fill_value=0)\n",
    "        .reset_index(level=1, drop=True)\n",
    "        .reset_index())\n",
    "    \n",
    "\n",
    "    # Reduce number of rows by\n",
    "    rol = max_window//data_every\n",
    "\n",
    "    df_label = df[[\"Sample\"]].iloc[rol - 1::rol, :]\n",
    "    df = df.drop([\"Sample\"], axis=1).rolling(rol).mean() \n",
    "\n",
    "    df = df.iloc[rol - 1::rol, :]\n",
    "    \n",
    "    df = pd.concat([df_label, df], axis = 1).reset_index(drop=True)\n",
    "                \n",
    "    # Group every max_window rows\n",
    "    s = df.groupby(['Sample']).cumcount()\n",
    "\n",
    "    df1 = df.set_index(['Sample', s]).unstack().sort_index(level=1, axis=1)\n",
    "    df1.columns = [f'{x}{y}' for x, y in df1.columns]\n",
    "    df1 = df1.reset_index()\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_drop = [\"energy\"]\n",
    "features_train_remove = [\"Frequency\", \"path_idx\", \"watch_loc\", \"Length\", \"Steps\", \"STANDING\", \"Timestamps\", \"WALKING\",\t\"RUNNING\",\t\"CYCLING\"]\n",
    "\n",
    "features_watch_remove = [\"altitude\", \"Steps\", \"StepsWindow\", \"mx\", \"my\", \"mz\"]\n",
    "features_activ_remove = [\"altitude\", \"mx\", \"my\", \"mz\"]\n",
    "\n",
    "features_path_remove = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\"]\n",
    "measures_drop_path = [\"energy\", \"max_freq\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_watch = pickle.load(open(\"group33_model_watch.pkl\", \"rb\"))\n",
    "clf_path =  pickle.load(open(\"group33_model_path.pkl\", \"rb\"))\n",
    "clf_activ = pickle.load(open(\"group33_model_activ.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path for all test traces\n",
    "dir_traces = \"./data/test/\"#'/kaggle/input/24-exercise2/data/test'\n",
    "\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f))]\n",
    "filenames.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': 1, 'watch_loc': 0, 'path_idx': 4, 'standing': False, 'walking': False, 'running': True, 'cycling': False, 'step_count': 336}\n",
      "We have elaborated 1 samples\n",
      "{'Id': 31, 'watch_loc': 0, 'path_idx': 1, 'standing': False, 'walking': True, 'running': True, 'cycling': False, 'step_count': 507}\n",
      "We have elaborated 31 samples\n",
      "{'Id': 61, 'watch_loc': 2, 'path_idx': 0, 'standing': False, 'walking': True, 'running': False, 'cycling': False, 'step_count': 1073}\n",
      "We have elaborated 61 samples\n",
      "{'Id': 91, 'watch_loc': 0, 'path_idx': 1, 'standing': False, 'walking': True, 'running': False, 'cycling': False, 'step_count': 711}\n",
      "We have elaborated 91 samples\n",
      "{'Id': 121, 'watch_loc': 1, 'path_idx': 0, 'standing': False, 'walking': True, 'running': False, 'cycling': False, 'step_count': 1289}\n",
      "We have elaborated 121 samples\n",
      "{'Id': 151, 'watch_loc': 2, 'path_idx': 3, 'standing': False, 'walking': True, 'running': True, 'cycling': False, 'step_count': 341}\n",
      "We have elaborated 151 samples\n",
      "{'Id': 181, 'watch_loc': 2, 'path_idx': 1, 'standing': False, 'walking': True, 'running': False, 'cycling': True, 'step_count': 242}\n",
      "We have elaborated 181 samples\n",
      "{'Id': 211, 'watch_loc': 2, 'path_idx': 4, 'standing': False, 'walking': True, 'running': False, 'cycling': False, 'step_count': 695}\n",
      "We have elaborated 211 samples\n",
      "{'Id': 241, 'watch_loc': 2, 'path_idx': 4, 'standing': False, 'walking': True, 'running': False, 'cycling': False, 'step_count': 588}\n",
      "We have elaborated 241 samples\n",
      "{'Id': 271, 'watch_loc': 2, 'path_idx': 4, 'standing': False, 'walking': True, 'running': False, 'cycling': False, 'step_count': 877}\n",
      "We have elaborated 271 samples\n"
     ]
    }
   ],
   "source": [
    "# Loop through all filenames to process recordings (uncomment next # to run the code and compute features on the fly (slow))\n",
    "submission = []\n",
    "LOW_COR_FEATURES = ['ax_gx_corr', 'ax_gy_corr', 'ax_gz_corr', 'ax_mx_corr', 'ax_my_corr', 'ax_mz_corr', 'ax_altitude_corr', 'ay_gx_corr', 'ay_gy_corr', 'ay_gz_corr', 'ay_mx_corr', 'ay_my_corr', 'ay_mz_corr', 'ay_altitude_corr', 'az_gy_corr', 'az_gz_corr', 'az_mx_corr', 'az_my_corr', 'az_mz_corr', 'az_altitude_corr', 'gx_mx_corr', 'gx_my_corr', 'gx_mz_corr', 'gx_altitude_corr', 'gy_mx_corr', 'gy_my_corr', 'gy_mz_corr', 'gy_altitude_corr', 'gz_mx_corr', 'gz_my_corr', 'gz_mz_corr', 'gz_altitude_corr']\n",
    "PRELOAD_FEATURES = \"group33_features_test.csv\"\n",
    "df_test = pd.read_csv(PRELOAD_FEATURES)\n",
    "\n",
    "# Remove not highly correlated features\n",
    "df_test = df_test.drop(LOW_COR_FEATURES, axis=1)\n",
    "\n",
    "# Compute df for activities\n",
    "steps = df_test.groupby('Sample').max()[[\"Steps\"]]\n",
    "df_filt = df_test.drop([\"Frequency\", \"Length\", \"Timestamps\", \"Steps\"], axis=1)\n",
    "\n",
    "df_filt = drop_features(df_filt, features_activ_remove, features_activ_remove, measures_drop)\n",
    "df_features_samples = features_per_sample(df_filt)\n",
    "X_active =  pd.concat([df_features_samples, steps[[\"Steps\"]]], axis=1)\n",
    "\n",
    "pred_activ = clf_activ.predict(X_active)\n",
    "\n",
    "# Compute df for path\n",
    "steps = df_test.groupby('Sample').max()[[\"Steps\"]]\n",
    "df_filt = df_test.drop([\"Frequency\", \"Length\", \"Timestamps\", \"Steps\"], axis=1)\n",
    "\n",
    "df_filt = drop_features(df_filt, features_path_remove, features_path_remove, measures_drop_path)\n",
    "# df_features_samples = features_per_sample(df_filt)\n",
    "df_features_samples = extend_df_windows(df_filt).set_index(\"Sample\")\n",
    "\n",
    "X_path =  pd.concat([df_features_samples, steps[[\"Steps\"]]], axis=1)\n",
    "\n",
    "pred_path = clf_path.predict(X_path)\n",
    "\n",
    "# Compute df for watch\n",
    "steps = df_test.groupby('Sample').max()[[\"Steps\"]]\n",
    "df_filt = df_test.drop([\"Frequency\", \"Length\", \"Timestamps\", \"Steps\"], axis=1)\n",
    "df_filt = drop_features(df_filt, features_watch_remove, features_watch_remove, measures_drop)\n",
    "\n",
    "df_features_samples = features_per_sample(df_filt)\n",
    "X_watch =  pd.concat([df_features_samples, steps[[\"Steps\"]]], axis=1)\n",
    "\n",
    "pred_watch = clf_watch.predict(X_watch)\n",
    "\n",
    "\n",
    "#data = pd.DataFrame()\n",
    "for idx, filename in enumerate(filenames):    \n",
    "    # Assumes filename format ends with a three-digit ID before \".pkl\"\n",
    "    match = re.search(r'(\\d{3})\\.pkl$', filename)\n",
    "    if match:\n",
    "        id = int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f'Filename {filename} does not match expected format')    \n",
    "\n",
    "    # Get number steps\n",
    "    sign, _ = get_signal_walk(filename)\n",
    "    nr_steps = algo_walk(sign, show=False)\n",
    "\n",
    "    pred_activ_sample = pred_activ[idx]\n",
    "\n",
    "    # Get path\n",
    "    pred_path_sample = pred_path[idx]\n",
    "\n",
    "    # Get watch loc\n",
    "    pred_watch_sample = pred_watch[idx]\n",
    "    \n",
    "    # Placeholder for the algorithm to process the recording\n",
    "    # Implement the logic to infer watch location, path index, step count,\n",
    "    # and activities (standing, walking, running, cycling) here.\n",
    "    # Ensure your algorithm is tolerant to missing data and does not crash\n",
    "    # when optional smartphone data traces are missing.\n",
    "\n",
    "    path_idx = pred_path_sample  # Integer, path in {0, 1, 2, 3, 4}\n",
    "    watch_loc = pred_watch_sample  # Integer, 0: left wrist, 1: belt, 2: right ankle\n",
    "    standing = bool(pred_activ_sample[0])  # Boolean, True if participant was standing still throughout the recording\n",
    "    walking = bool(pred_activ_sample[1])  # Boolean, True if participant was walking throughout the recording\n",
    "    running = bool(pred_activ_sample[2])  # Boolean, True if participant was running throughout the recording\n",
    "    cycling = bool(pred_activ_sample[3])  # Boolean, True if participant was cycling throughout the recording\n",
    "    step_count = nr_steps  # Integer, number of steps, must be provided for each recording\n",
    "\n",
    "    predictions = {\n",
    "        'Id': id, \n",
    "        'watch_loc': watch_loc, \n",
    "        'path_idx': path_idx,\n",
    "        'standing': standing,\n",
    "        'walking': walking,\n",
    "        'running': running,\n",
    "        'cycling': cycling,\n",
    "        'step_count': step_count\n",
    "        }\n",
    "\n",
    "    submission.append(predictions)\n",
    "\n",
    "    if idx % 30 == 1:\n",
    "        print(predictions)\n",
    "        print(\"We have elaborated {} samples\".format(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predicted values into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using a text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns of the .csv file (\"Id\", \"watch_loc\", \"path_idx\", \"standing\", \"walking\", \"running\", \"cycling\", \"step_count\")\n",
    "submission_df = pd.DataFrame(submission, columns=['Id', 'watch_loc', 'path_idx', 'standing', 'walking', 'running', 'cycling', 'step_count'])\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
